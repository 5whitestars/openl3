{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load functions from openl3/cli.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# %load ../../openl3/cli.py\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import sklearn.decomposition\n",
    "from openl3 import process_audio_file, process_image_file, process_video_file\n",
    "from openl3.models import load_audio_embedding_model, load_image_embedding_model\n",
    "from openl3.openl3_exceptions import OpenL3Error\n",
    "from argparse import ArgumentParser, RawDescriptionHelpFormatter, ArgumentTypeError\n",
    "from collections import Iterable\n",
    "from six import string_types\n",
    "\n",
    "\n",
    "def positive_float(value):\n",
    "    \"\"\"An argparse type method for accepting only positive floats\"\"\"\n",
    "    try:\n",
    "        fvalue = float(value)\n",
    "    except (ValueError, TypeError) as e:\n",
    "        raise ArgumentTypeError('Expected a positive float, error message: '\n",
    "                                '{}'.format(e))\n",
    "    if fvalue <= 0:\n",
    "        raise ArgumentTypeError('Expected a positive float')\n",
    "    return fvalue\n",
    "\n",
    "\n",
    "def get_file_list(input_list):\n",
    "    \"\"\"Get list of files from the list of inputs\"\"\"\n",
    "    if not isinstance(input_list, Iterable) or isinstance(input_list, string_types):\n",
    "        raise ArgumentTypeError('input_list must be iterable (and not string)')\n",
    "    file_list = []\n",
    "    for item in input_list:\n",
    "        if os.path.isfile(item):\n",
    "            file_list.append(os.path.abspath(item))\n",
    "        elif os.path.isdir(item):\n",
    "            for fname in os.listdir(item):\n",
    "                path = os.path.join(item, fname)\n",
    "                if os.path.isfile(path):\n",
    "                    file_list.append(path)\n",
    "        else:\n",
    "            raise OpenL3Error('Could not find {}'.format(item))\n",
    "\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def run(modality, inputs, output_dir=None, suffix=None,\n",
    "        input_repr=\"mel256\", content_type=\"music\",\n",
    "        audio_embedding_size=6144, audio_center=True, audio_hop_size=0.1,\n",
    "        image_embedding_size=8192, verbose=False):\n",
    "    \"\"\"\n",
    "    Computes and saves L3 embedding for given inputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    modality : str\n",
    "        String to specify the modalities to be processed: audio, image, or video\n",
    "    inputs : list of str, or str\n",
    "        File/directory path or list of file/directory paths to be processed\n",
    "    output_dir : str or None\n",
    "        Path to directory for saving output files. If None, output files will\n",
    "        be saved to the directory containing the input file.\n",
    "    suffix : str or None\n",
    "        String to be appended to the output filename, i.e. <base filename>_<suffix>.npy.\n",
    "        If None, then no suffix will be added, i.e. <base filename>.npy.\n",
    "    input_repr : \"linear\", \"mel128\", or \"mel256\"\n",
    "        Spectrogram representation used for model.\n",
    "    content_type : \"music\" or \"env\"\n",
    "        Type of content used to train embedding.\n",
    "    audio_embedding_size : 6144 or 512\n",
    "        Audio embedding dimensionality.\n",
    "    audio_center : boolean\n",
    "        If True, pads beginning of signal so timestamps correspond\n",
    "        to center of window.\n",
    "    audio_hop_size : float\n",
    "        Hop size in seconds.\n",
    "    image_embedding_size : 8192 or 512\n",
    "        Embedding dimensionality.\n",
    "    verbose : boolean\n",
    "        If True, print verbose messages.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(inputs, string_types):\n",
    "        file_list = [inputs]\n",
    "    elif isinstance(inputs, Iterable):\n",
    "        file_list = get_file_list(inputs)\n",
    "    else:\n",
    "        raise OpenL3Error('Invalid input: {}'.format(str(inputs)))\n",
    "\n",
    "    if len(file_list) == 0:\n",
    "        print('openl3: No files found in {}. Aborting.'.format(str(inputs)))\n",
    "        sys.exit(-1)\n",
    "\n",
    "    # Load model\n",
    "    if modality == 'audio':\n",
    "        model = load_audio_embedding_model(input_repr, content_type,\n",
    "                                           audio_embedding_size)\n",
    "\n",
    "        # Process all files in the arguments\n",
    "        for filepath in file_list:\n",
    "            if verbose:\n",
    "                print('openl3: Processing: {}'.format(filepath))\n",
    "            process_audio_file(filepath,\n",
    "                               output_dir=output_dir,\n",
    "                               suffix=suffix,\n",
    "                               model=model,\n",
    "                               center=audio_center,\n",
    "                               hop_size=audio_hop_size,\n",
    "                               verbose=verbose)\n",
    "    elif modality == 'image':\n",
    "        model = load_image_embedding_model(input_repr, content_type,\n",
    "                                           image_embedding_size)\n",
    "\n",
    "        # Process all files in the arguments\n",
    "        for filepath in file_list:\n",
    "            if verbose:\n",
    "                print('openl3: Processing: {}'.format(filepath))\n",
    "            process_image_file(filepath,\n",
    "                               output_dir=output_dir,\n",
    "                               suffix=suffix,\n",
    "                               model=model,\n",
    "                               verbose=verbose)\n",
    "    elif modality == 'video':\n",
    "        audio_model = load_audio_embedding_model(input_repr, content_type,\n",
    "                                                 audio_embedding_size)\n",
    "        image_model = load_image_embedding_model(input_repr, content_type,\n",
    "                                                 image_embedding_size)\n",
    "\n",
    "        # Process all files in the arguments\n",
    "        for filepath in file_list:\n",
    "            if verbose:\n",
    "                print('openl3: Processing: {}'.format(filepath))\n",
    "            process_video_file(filepath,\n",
    "                               output_dir=output_dir,\n",
    "                               suffix=suffix,\n",
    "                               audio_model=audio_model,\n",
    "                               image_model=image_model,\n",
    "                               audio_embedding_size=audio_embedding_size,\n",
    "                               audio_center=audio_center,\n",
    "                               audio_hop_size=audio_hop_size,\n",
    "                               image_embedding_size=image_embedding_size,\n",
    "                               verbose=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        print('openl3: Done!')\n",
    "\n",
    "\n",
    "def parse_args(args):\n",
    "    parser = ArgumentParser(sys.argv[0], description=main.__doc__,\n",
    "                            formatter_class=RawDescriptionHelpFormatter)\n",
    "\n",
    "    parser.add_argument('modality',\n",
    "                        choices=['audio', 'image', 'video'],\n",
    "                        help='String to specify the modality to the '\n",
    "                             'embedding model, audio, image, or video.')\n",
    "\n",
    "    parser.add_argument('inputs', nargs='+',\n",
    "                        help='Path or paths to files to process, or path to '\n",
    "                             'a directory of files to process.')\n",
    "\n",
    "    parser.add_argument('--output-dir', '-o', default=None,\n",
    "                        help='Directory to save the ouptut file(s); '\n",
    "                             'if not given, the output will be '\n",
    "                             'saved to the same directory as the input WAV '\n",
    "                             'file(s).')\n",
    "\n",
    "    parser.add_argument('--suffix', '-x', default=None,\n",
    "                        help='String to append to the output filenames.'\n",
    "                             'If not provided, no suffix is added.')\n",
    "\n",
    "    parser.add_argument('--input-repr', '-i', default='mel256',\n",
    "                        choices=['linear', 'mel128', 'mel256'],\n",
    "                        help='String specifying the time-frequency input '\n",
    "                             'representation for the audio embedding model.')\n",
    "\n",
    "    parser.add_argument('--content-type', '-c', default='music',\n",
    "                        choices=['music', 'env'],\n",
    "                        help='Content type used to train embedding model.')\n",
    "\n",
    "    parser.add_argument('--audio-embedding-size', '-as', type=int, default=6144,\n",
    "                        choices=[6144, 512],\n",
    "                        help='Audio embedding dimensionality.')\n",
    "\n",
    "    parser.add_argument('--no-audio-centering', '-n', action='store_true',\n",
    "                        default=False,\n",
    "                        help='Used for audio embeddings. Do not pad signal; '\n",
    "                             'timestamps will correspond to '\n",
    "                             'the beginning of each analysis window.')\n",
    "\n",
    "    parser.add_argument('--audio-hop-size', '-t', type=positive_float, default=0.1,\n",
    "                        help='Used for audio embeddings. '\n",
    "                             'Hop size in seconds for processing audio files.')\n",
    "\n",
    "    parser.add_argument('--image-embedding-size', '-is', type=int, default=8192,\n",
    "                        choices=[8192, 512],\n",
    "                        help='Image embedding dimensionality.')\n",
    "\n",
    "    parser.add_argument('--quiet', '-q', action='store_true', default=False,\n",
    "                        help='Suppress all non-error messages to stdout.')\n",
    "\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Extracts audio embeddings from models based on the Look, Listen, and Learn models (Arandjelovic and Zisserman 2017).\n",
    "    \"\"\"\n",
    "    args = parse_args(sys.argv[1:])\n",
    "\n",
    "    run(args.modality,\n",
    "        args.inputs,\n",
    "        output_dir=args.output_dir,\n",
    "        suffix=args.suffix,\n",
    "        input_repr=args.input_repr,\n",
    "        content_type=args.content_type,\n",
    "        audio_embedding_size=args.audio_embedding_size,\n",
    "        audio_center=not args.no_audio_centering,\n",
    "        audio_hop_size=args.audio_hop_size,\n",
    "        image_embedding_size=args.image_embedding_size,\n",
    "        verbose=not args.quiet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to wav file\n",
    "test_path = os.path.expanduser('..')\n",
    "chirp44_path = os.path.join(test_path, 'data', 'audio', 'chirp_44k.wav')\n",
    "daisy_path = os.path.join(test_path, 'data', 'image', 'daisy.jpg')\n",
    "bento_path = os.path.join(test_path, 'data', 'image', 'bento.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to store output embeddings\n",
    "output_dir = os.path.expanduser('~/openl3_output/')\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jsondotload/anaconda3/envs/openl3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# compute mel256/music/6144 regression audio embedding\n",
    "suffix=None\n",
    "input_repr='mel256'\n",
    "content_type='music'\n",
    "audio_embedding_size=6144\n",
    "center=True\n",
    "hop_size=0.1\n",
    "verbose=False\n",
    "\n",
    "run('audio',\n",
    "    chirp44_path,\n",
    "    output_dir=output_dir,\n",
    "    suffix=suffix,\n",
    "    input_repr=input_repr,\n",
    "    content_type=content_type,\n",
    "    audio_embedding_size=audio_embedding_size,\n",
    "    audio_center=center,\n",
    "    audio_hop_size=hop_size,\n",
    "    verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute linear/env/512 regression audio embedding\n",
    "suffix='linear'\n",
    "input_repr='linear'\n",
    "content_type='env'\n",
    "audio_embedding_size=512\n",
    "center=False\n",
    "hop_size=0.5\n",
    "verbose=False\n",
    "\n",
    "run('audio',\n",
    "    chirp44_path,\n",
    "    output_dir=output_dir,\n",
    "    suffix=suffix,\n",
    "    input_repr=input_repr,\n",
    "    content_type=content_type,\n",
    "    audio_embedding_size=audio_embedding_size,\n",
    "    audio_center=center,\n",
    "    audio_hop_size=hop_size,\n",
    "    verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mel256/music/8192 regression image embedding\n",
    "suffix=None\n",
    "input_repr='mel256'\n",
    "content_type='music'\n",
    "image_embedding_size=8192\n",
    "verbose=False\n",
    "\n",
    "run('image',\n",
    "    daisy_path,\n",
    "    output_dir=output_dir,\n",
    "    suffix=suffix,\n",
    "    input_repr=input_repr,\n",
    "    content_type=content_type,\n",
    "    image_embedding_size=image_embedding_size,\n",
    "    verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute linear/env/512 regression image embedding\n",
    "suffix='linear'\n",
    "input_repr='linear'\n",
    "content_type='env'\n",
    "image_embedding_size=512\n",
    "verbose=False\n",
    "\n",
    "run('image',\n",
    "    daisy_path,\n",
    "    output_dir=output_dir,\n",
    "    suffix=suffix,\n",
    "    input_repr=input_repr,\n",
    "    content_type=content_type,\n",
    "    image_embedding_size=image_embedding_size,\n",
    "    verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "7/7 [==============================] - 2s 273ms/step\n"
     ]
    }
   ],
   "source": [
    "# compute mel256/music regression audio (6144) and image (8192) embeddings\n",
    "suffix=None\n",
    "input_repr='mel256'\n",
    "content_type='music'\n",
    "image_embedding_size=8192\n",
    "audio_embedding_size=6144\n",
    "center=True\n",
    "hop_size=0.1\n",
    "verbose=False\n",
    "\n",
    "run('video',\n",
    "    bento_path,\n",
    "    output_dir=output_dir,\n",
    "    suffix=suffix,\n",
    "    input_repr=input_repr,\n",
    "    content_type=content_type,\n",
    "    image_embedding_size=image_embedding_size,\n",
    "    audio_embedding_size=audio_embedding_size,\n",
    "    audio_center=center,\n",
    "    audio_hop_size=hop_size,    \n",
    "    verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2/2 [==============================] - 1s 403ms/step\n"
     ]
    }
   ],
   "source": [
    "# compute linear/env regression audio (512) and image (512) embeddings\n",
    "suffix='linear'\n",
    "input_repr='linear'\n",
    "content_type='env'\n",
    "image_embedding_size=512\n",
    "audio_embedding_size=512\n",
    "center=False\n",
    "hop_size=0.5\n",
    "verbose=False\n",
    "\n",
    "run('video',\n",
    "    bento_path,\n",
    "    output_dir=output_dir,\n",
    "    suffix=suffix,\n",
    "    input_repr=input_repr,\n",
    "    content_type=content_type,\n",
    "    image_embedding_size=image_embedding_size,\n",
    "    audio_embedding_size=audio_embedding_size,\n",
    "    audio_center=center,\n",
    "    audio_hop_size=hop_size,    \n",
    "    verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: compare to previous regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_audio_emb_path = os.path.join(test_path, 'data', 'regression', 'chirp_44k.npz')\n",
    "reg_audio_emb_linear_path = os.path.join(test_path, 'data', 'regression', 'chirp_44k_linear.npz')\n",
    "\n",
    "new_audio_emb_path = os.path.join(output_dir, 'chirp_44k.npz')\n",
    "new_audio_emb_linear_path = os.path.join(output_dir, 'chirp_44k_linear.npz')\n",
    "\n",
    "reg_image_emb_path = os.path.join(test_path, 'data', 'regression', 'daisy.npz')\n",
    "reg_image_emb_linear_path = os.path.join(test_path, 'data', 'regression', 'daisy_linear.npz')\n",
    "\n",
    "new_image_emb_path = os.path.join(output_dir, 'daisy.npz')\n",
    "new_image_emb_linear_path = os.path.join(output_dir, 'daisy_linear.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_audio_emb = np.load(reg_audio_emb_path)\n",
    "reg_audio_emb_linear = np.load(reg_audio_emb_linear_path)\n",
    "\n",
    "new_audio_emb = np.load(new_audio_emb_path)\n",
    "new_audio_emb_linear = np.load(new_audio_emb_linear_path)\n",
    "\n",
    "reg_image_emb = np.load(reg_image_emb_path)\n",
    "reg_image_emb_linear = np.load(reg_image_emb_linear_path)\n",
    "\n",
    "new_image_emb = np.load(new_image_emb_path)\n",
    "new_image_emb_linear = np.load(new_image_emb_linear_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(reg_audio_emb['timestamps'], new_audio_emb['timestamps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(reg_audio_emb['embedding'], new_audio_emb['embedding'], rtol=1e-05, atol=1e-06, equal_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(reg_image_emb['embedding'], new_image_emb['embedding'], rtol=1e-05, atol=1e-06, equal_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(reg_audio_emb_linear['timestamps'], new_audio_emb_linear['timestamps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(reg_audio_emb_linear['embedding'], new_audio_emb_linear['embedding'], rtol=1e-05, atol=1e-06, equal_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(reg_image_emb_linear['embedding'], new_image_emb_linear['embedding'], rtol=1e-05, atol=1e-06, equal_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video regression\n",
    "reg_audio_emb_path = os.path.join(test_path, 'data', 'regression', 'bento_audio.npz')\n",
    "reg_audio_emb_linear_path = os.path.join(test_path, 'data', 'regression', 'bento_audio_linear.npz')\n",
    "\n",
    "new_audio_emb_path = os.path.join(output_dir, 'bento_audio.npz')\n",
    "new_audio_emb_linear_path = os.path.join(output_dir, 'bento_audio_linear.npz')\n",
    "\n",
    "reg_image_emb_path = os.path.join(test_path, 'data', 'regression', 'bento_image.npz')\n",
    "reg_image_emb_linear_path = os.path.join(test_path, 'data', 'regression', 'bento_image_linear.npz')\n",
    "\n",
    "new_image_emb_path = os.path.join(output_dir, 'bento_image.npz')\n",
    "new_image_emb_linear_path = os.path.join(output_dir, 'bento_image_linear.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_audio_emb = np.load(reg_audio_emb_path)\n",
    "reg_audio_emb_linear = np.load(reg_audio_emb_linear_path)\n",
    "\n",
    "new_audio_emb = np.load(new_audio_emb_path)\n",
    "new_audio_emb_linear = np.load(new_audio_emb_linear_path)\n",
    "\n",
    "reg_image_emb = np.load(reg_image_emb_path)\n",
    "reg_image_emb_linear = np.load(reg_image_emb_linear_path)\n",
    "\n",
    "new_image_emb = np.load(new_image_emb_path)\n",
    "new_image_emb_linear = np.load(new_image_emb_linear_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(reg_audio_emb['timestamps'], new_audio_emb['timestamps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(reg_image_emb['timestamps'], new_image_emb['timestamps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(reg_audio_emb['embedding'], new_audio_emb['embedding'], rtol=1e-05, atol=1e-06, equal_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(reg_image_emb['embedding'], new_image_emb['embedding'], rtol=1e-05, atol=1e-06, equal_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-dc04ef450b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_audio_emb_linear\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_audio_emb_linear\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/openl3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mallclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m     \"\"\"\n\u001b[0;32m-> 2423\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequal_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mequal_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2424\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/openl3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36misclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2522\u001b[0m     \u001b[0myfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2525\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m         \u001b[0mfinite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxfin\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0myfin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/openl3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mwithin_tol\u001b[0;34m(x, y, atol, rtol)\u001b[0m\n\u001b[1;32m   2508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2509\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2510\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mless_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrtol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2512\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,) (2,) "
     ]
    }
   ],
   "source": [
    "assert np.allclose(reg_audio_emb_linear['timestamps'], new_audio_emb_linear['timestamps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(reg_image_emb_linear['timestamps'], new_image_emb_linear['timestamps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(reg_audio_emb_linear['embedding'], new_audio_emb_linear['embedding'], rtol=1e-05, atol=1e-06, equal_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(reg_image_emb_linear['embedding'], new_image_emb_linear['embedding'], rtol=1e-05, atol=1e-06, equal_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
